<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>AffordGrasp</title>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());

        gtag('config', 'G-PYVRSFMDRL');
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="static/images/favicon.png">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title">AffordGrasp: In-Context Affordance Reasoning for
                        Open-Vocabulary Task-Oriented Grasping in Clutter</h1>
                    <div class="is-size-5 publication-authors">
            <span class="author-block">
              Yingbo Tang<sup>1</sup>&nbsp; &nbsp; &nbsp;
            </span>
                        <span class="author-block">
              Shuaike Zhang<sup>2</sup>&nbsp; &nbsp; &nbsp;
            </span>
                        <span class="author-block">
              Xiaoshuai Hao<sup>3</sup>&nbsp; &nbsp; &nbsp;
            </span>
                        <span class="author-block">
              Pengwei Wang<sup>3</sup>
            </span>
                    </div>

                    <div class="is-size-5 publication-authors">
            <span class="author-block">
              Jianlong Wu<sup>2</sup>&nbsp; &nbsp; &nbsp;
            </span>
                        <span class="author-block">
              Zhongyuan Wang<sup>3</sup>&nbsp; &nbsp; &nbsp;
            </span>
                        <span class="author-block">
              Shanghang Zhang<sup>3,4</sup>&nbsp; &nbsp; &nbsp;
            </span>
                    </div>

                    <br>

                    <!--  Apartment        -->
                    <div class="is-size-5 publication-authors">
            <span class="author-block">
              <sup>1</sup>Institute of Automation, Chinese Academy of Sciences&nbsp;&nbsp;&nbsp; <sup>2</sup>Harbin Institute of Technology (Shenzhen)&nbsp;&nbsp;&nbsp;
              <sup>3</sup>Beijing Academy of Artificial Intelligence&nbsp;&nbsp;&nbsp; <sup>4</sup>State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University
            </span>
                    </div>

                    <div class="column has-text-centered">
                        <div class="publication-links">
                            <!-- PDF Link. -->
                            <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
                            <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
                            <!-- Code Link. -->
                            <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
                        </div>


                    </div>
                </div>
            </div>
        </div>
    </div>
</section>


<section class="section">
    <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-2">Abstract</h2>
                <div class="content has-text-justified">
                    <p>
                        Inferring the affordance of an object and grasping it in a task-oriented manner is crucial for
                        robots to successfully complete manipulation tasks.
                        Affordance indicates where and how to grasp an object by taking its functionality into account,
                        serving as the foundation for effective task-oriented grasping.
                        However, current task-oriented methods often depend on extensive training data that is confined
                        to specific tasks and objects, making it difficult to generalize to novel objects and complex
                        scenes.
                        In this paper, we introduce <b>AffordGrasp</b>, a novel open-vocabulary grasping
                        framework that leverages the reasoning capabilities of vision-language models (VLMs) for
                        in-context affordance reasoning. Unlike existing methods that rely on explicit task and object
                        specifications, our approach infers tasks directly from implicit user instructions, enabling
                        more intuitive and seamless human-robot interaction in everyday scenarios.
                        Building on the reasoning outcomes, our framework identifies task-relevant objects and grounds
                        their part-level affordances using a visual grounding module. This allows us to generate
                        task-oriented grasp poses precisely within the affordance regions of the object, ensuring both
                        functional and context-aware robotic manipulation.
                        Extensive experiments demonstrate that <b>AffordGrasp</b> achieves state-of-the-art
                        performance in both simulation and real-world scenarios, highlighting the effectiveness of our
                        method. We believe our approach advances robotic manipulation techniques and contributes to the
                        broader field of embodied AI.
                    </p>
                </div>
            </div>
        </div>

        <hr>

        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-2">Framework</h2>
                <div class="pipeline">
                    <img
                            src="./static/images/figure_framework.png"
                            class="pipeline image"
                            alt="pipeline image"
                    />
                </div>
                <div class="content has-text-justified">
                    <br>
                    <p>
                        <b>Overall Framework of AffordGrasp.</b>
                        The framework processes user instructions and RGB-D scene observations to achieve
                        open-vocabulary task-oriented grasping in clutter. We leverage GPT-4o for in-context affordance
                        reasoning, decomposing the process into three steps:
                        (1) Extracting the task goal and functional requirements from implicit user instructions (e.g.,
                        "I want to scoop something").
                        (2) Identifying the most task-relevant object in the RGB image (e.g., a wooden spoon).
                        (3) Decomposing the object into functional parts and selecting the optimal graspable part (e.g.,
                        the handle) based on its affordances.
                        Based on the reasoning results, a visual affordance grounding module grounds the inferred object
                        and part affordances into pixel-level masks. With the affordance mask and RGB-D images, we
                        employ AnyGrasp to generate task-oriented 6D grasp poses on the target part.
                    </p>
                </div>
            </div>
        </div>
        <hr>
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-2">RealWorld Experiment Results</h2>
            </div>
        </div>
        <br>
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <div class="pipeline">
                    <img
                            src="./static/images/TABLE3.png"
                            class="pipeline image"
                            alt="pipeline image"
                    />
                </div>
                <p>
                    <b>COMPARISON RESULTS OF DIFFERENT METHODS IN REAL-WORLD EXPERIMENTS</b>
                </p>
            </div>
        </div>
        <br>
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <div class="pipeline">
                    <img
                            src="./static/images/figure_realworld.png"
                            class="pipeline image"
                            alt="pipeline image"
                    />
                </div>
                <br>
                <p>
                    The examples in <b>real-world experiments</b>, where the visualization of affordance grounding and
                    task-oriented grasp pose generation are provided.
                </p>
            </div>
        </div>
        <hr>
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-2">RealWorld Experiment Videos</h2>
                <div class="item"
                     style="display: flex; justify-content: center; align-items: center; gap: 15%;padding: 0 25%">
                    <!-- 视频 1 -->
                    <video id="bottle"  controls muted loop playsinline
                           style="width: 100%; height: auto; max-width: 100%;">
                        <source src="./static/videos/iros_demo/bottle.mp4" type="video/mp4">
                    </video>
                    <!-- 视频 2 -->
                    <video id="kettle"  controls muted loop playsinline
                           style="width: 100%; height: auto; max-width: 100%;">
                        <source src="./static/videos/iros_demo/kettle.mp4" type="video/mp4">
                    </video>
                    <!-- 视频 3 -->
                    <video id="knife"  controls muted loop playsinline
                           style="width: 100%; height: auto; max-width: 100%;">
                        <source src="./static/videos/iros_demo/knife.mp4" type="video/mp4">
                    </video>
                </div>
                <br>
                <br>
                <div class="item"
                     style="display: flex; justify-content: center; align-items: center; gap: 15%;padding: 0 25%">
                    <!-- 视频 1 -->
                    <video id="mug"  controls muted loop playsinline
                           style="width: 100%; height: auto; max-width: 100%;">
                        <source src="./static/videos/iros_demo/mug.mp4" type="video/mp4">
                    </video>
                    <!-- 视频 2 -->
                    <video id="pan"  controls muted loop playsinline
                           style="width: 100%; height: auto; max-width: 100%;">
                        <source src="./static/videos/iros_demo/pan.mp4" type="video/mp4">
                    </video>
                    <!-- 视频 3 -->
                    <video id="racket"  controls muted loop playsinline
                           style="width: 100%; height: auto; max-width: 100%;">
                        <source src="./static/videos/iros_demo/racket.mp4" type="video/mp4">
                    </video>
                </div>
                <br>
                <br>
                <div class="item"
                     style="display: flex; justify-content: center; align-items: center; gap: 15%;padding: 0 25%">
                    <!-- 视频 1 -->
                    <video id="screwdriver"  controls muted loop playsinline
                           style="width: 100%; height: auto; max-width: 100%;">
                        <source src="./static/videos/iros_demo/screwdriver.mp4" type="video/mp4">
                    </video>
                    <!-- 视频 2 -->
                    <video id="spatula"  controls muted loop playsinline
                           style="width: 100%; height: auto; max-width: 100%;">
                        <source src="./static/videos/iros_demo/spatula.mp4" type="video/mp4">
                    </video>
                    <!-- 视频 3 -->
                    <video id="spoon"  controls muted loop playsinline
                           style="width: 100%; height: auto; max-width: 100%;">
                        <source src="./static/videos/iros_demo/spoon.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
        </div>
        <br>
        <hr>

        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-2">Simulation Experiment Results</h2>
            </div>
        </div>
        <br>
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <div class="pipeline">
                    <img
                            src="./static/images/TABLE1.png"
                            class="pipeline image"
                            alt="pipeline image"
                    />
                </div>
                <p>
                    SIMULATION RESULTS OF DIFFERENT METHODS GRASPING SINGLE OBJECT
                </p>
            </div>
        </div>
        <br>
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <div class="pipeline">
                    <img
                            src="./static/images/TABLE2.png"
                            class="pipeline image"
                            alt="pipeline image"
                    />
                </div>
                <p>
                    SIMULATION RESULTS OF DIFFERENT METHODS GRASPING IN CLUTTER
                </p>
            </div>
        </div>
        <br>
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <div class="pipeline">
                    <img
                            src="./static/images/figure_simulation.png"
                            class="pipeline image"
                            alt="pipeline image"
                    />
                </div>
                <p>
                    <br>
                    The cases of cluttered grasping in <b>simulation.</b>
                    <br>
                    The <b>affordances</b> of target object are labeled with red stars.
                </p>
            </div>
        </div>
        <hr>
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-2">Simulation Experiment Videos</h2>
                <div class="item"
                     style="display: flex; justify-content: center; align-items: center; gap: 15%;padding: 0 25%">
                    <!-- 视频 1 -->
                    <video id="simulation_hammer"  controls muted loop playsinline
                           style="width: 100%; height: auto; max-width: 100%;">
                        <source src="./static/videos/simulation_hammer.mp4" type="video/mp4">
                    </video>

                    <!-- 视频 2 -->
                    <video id="simulation_screwdriver"  controls muted loop playsinline
                           style="width: 100%; height: auto; max-width: 100%;">
                        <source src="./static/videos/simulation_screwdriver.mp4" type="video/mp4">
                    </video>

                    <!-- 视频 3 -->
                    <video id="simulation_spoon"  controls muted loop playsinline
                           style="width: 100%; height: auto; max-width: 100%;">
                        <source src="./static/videos/simulation_spoon.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
        </div>
    </div>
</section>

<footer class="footer">
    <div class="container">
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content" style="text-align: center">
                    <p>
                        The source code of this website is borrowed from
                        <a href="https://github.com/nerfies/nerfies.github.io">nerfies</a>.
                    </p>
                </div>
            </div>
        </div>
    </div>
</footer>

</body>
</html>
